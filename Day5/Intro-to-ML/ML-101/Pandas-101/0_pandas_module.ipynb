{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-the-library\" data-toc-modified-id=\"Import-the-library-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import the library</a></span></li></ul></li><li><span><a href=\"#Pandas-Dataframe\" data-toc-modified-id=\"Pandas-Dataframe-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pandas Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Columns-and-Row-index\" data-toc-modified-id=\"Columns-and-Row-index-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Columns and Row index</a></span></li><li><span><a href=\"#Series-objects\" data-toc-modified-id=\"Series-objects-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Series objects</a></span></li><li><span><a href=\"#Dataframe-objects\" data-toc-modified-id=\"Dataframe-objects-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Dataframe objects</a></span></li><li><span><a href=\"#Data-Types-comparison\" data-toc-modified-id=\"Data-Types-comparison-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Data Types comparison</a></span></li></ul></li><li><span><a href=\"#File-management\" data-toc-modified-id=\"File-management-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>File management</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data-from-CSV\" data-toc-modified-id=\"Load-data-from-CSV-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load data from CSV</a></span></li><li><span><a href=\"#Load-data-from-MS-Excel\" data-toc-modified-id=\"Load-data-from-MS-Excel-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Load data from MS Excel</a></span></li><li><span><a href=\"#Load-from-SAS\" data-toc-modified-id=\"Load-from-SAS-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Load from SAS</a></span></li><li><span><a href=\"#Load-data-from-SQL\" data-toc-modified-id=\"Load-data-from-SQL-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Load data from SQL</a></span></li><li><span><a href=\"#Save-DataFrame\" data-toc-modified-id=\"Save-DataFrame-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Save DataFrame</a></span></li></ul></li><li><span><a href=\"#Working-with-Dataframe\" data-toc-modified-id=\"Working-with-Dataframe-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Working with Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generic-methods\" data-toc-modified-id=\"Generic-methods-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Generic methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataFrame-attributes\" data-toc-modified-id=\"DataFrame-attributes-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>DataFrame attributes</a></span></li><li><span><a href=\"#Iteration-methods\" data-toc-modified-id=\"Iteration-methods-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Iteration methods</a></span></li></ul></li><li><span><a href=\"#Math-methods\" data-toc-modified-id=\"Math-methods-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Math methods</a></span></li><li><span><a href=\"#Select/filter-rows/cols\" data-toc-modified-id=\"Select/filter-rows/cols-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Select/filter rows/cols</a></span></li></ul></li><li><span><a href=\"#Access-the-Dataframe\" data-toc-modified-id=\"Access-the-Dataframe-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Access the Dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Column-wise-operation\" data-toc-modified-id=\"Column-wise-operation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Column-wise operation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Change-column-labels\" data-toc-modified-id=\"Change-column-labels-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Change column labels</a></span></li><li><span><a href=\"#Selecting-columns\" data-toc-modified-id=\"Selecting-columns-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Selecting columns</a></span></li><li><span><a href=\"#Adding-new-columns-to-a-DataFrame\" data-toc-modified-id=\"Adding-new-columns-to-a-DataFrame-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Adding new columns to a DataFrame</a></span></li><li><span><a href=\"#Common-column-element-wise-methods\" data-toc-modified-id=\"Common-column-element-wise-methods-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Common column element wise methods</a></span></li></ul></li><li><span><a href=\"#Rowwise-operations\" data-toc-modified-id=\"Rowwise-operations-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Rowwise operations</a></span></li><li><span><a href=\"#Summary--Dataframe-indexing\" data-toc-modified-id=\"Summary--Dataframe-indexing-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Summary  Dataframe indexing</a></span></li><li><span><a href=\"#Exercises-(30-min)\" data-toc-modified-id=\"Exercises-(30-min)-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Exercises (30 min)</a></span></li></ul></li><li><span><a href=\"#Advanced-operations\" data-toc-modified-id=\"Advanced-operations-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Advanced operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-join,-concatenate\" data-toc-modified-id=\"Merge-join,-concatenate-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Merge join, concatenate</a></span><ul class=\"toc-item\"><li><span><a href=\"#Database-style-DataFrame-joining/merging\" data-toc-modified-id=\"Database-style-DataFrame-joining/merging-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Database-style DataFrame joining/merging</a></span></li></ul></li><li><span><a href=\"#Groupby-:-split-apply-combine\" data-toc-modified-id=\"Groupby-:-split-apply-combine-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Groupby : split-apply-combine</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aggregation-functions\" data-toc-modified-id=\"Aggregation-functions-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Aggregation functions</a></span></li><li><span><a href=\"#Grouping-on-Continuous-Values\" data-toc-modified-id=\"Grouping-on-Continuous-Values-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Grouping on Continuous Values</a></span></li></ul></li><li><span><a href=\"#Working-with-missing-data\" data-toc-modified-id=\"Working-with-missing-data-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Working with missing data</a></span></li><li><span><a href=\"#Exercises-(15-min)\" data-toc-modified-id=\"Exercises-(15-min)-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Exercises (15 min)</a></span></li></ul></li><li><span><a href=\"#Reshaping\" data-toc-modified-id=\"Reshaping-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Reshaping</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pivot\" data-toc-modified-id=\"Pivot-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Pivot</a></span></li><li><span><a href=\"#Stack-/-Unstack\" data-toc-modified-id=\"Stack-/-Unstack-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Stack / Unstack</a></span></li><li><span><a href=\"#Melt\" data-toc-modified-id=\"Melt-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Melt</a></span></li><li><span><a href=\"#Crosstab\" data-toc-modified-id=\"Crosstab-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Crosstab</a></span></li><li><span><a href=\"#Indicator-/-dummy-variables\" data-toc-modified-id=\"Indicator-/-dummy-variables-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Indicator / dummy variables</a></span></li></ul></li><li><span><a href=\"#Working-with-timeseries\" data-toc-modified-id=\"Working-with-timeseries-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Working with timeseries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Timestamp-vs-Span\" data-toc-modified-id=\"Timestamp-vs-Span-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Timestamp vs Span</a></span></li><li><span><a href=\"#DatetimeIndex-and-Periodindex\" data-toc-modified-id=\"DatetimeIndex-and-Periodindex-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>DatetimeIndex and Periodindex</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pandas-Period-string\" data-toc-modified-id=\"Pandas-Period-string-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span><em>Pandas Period string</em></a></span></li></ul></li><li><span><a href=\"#Resampling-frequency\" data-toc-modified-id=\"Resampling-frequency-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Resampling frequency</a></span></li><li><span><a href=\"#Row-selection-with-timeseries\" data-toc-modified-id=\"Row-selection-with-timeseries-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Row selection with timeseries</a></span></li><li><span><a href=\"#Exercises-(20-min)\" data-toc-modified-id=\"Exercises-(20-min)-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Exercises (20 min)</a></span></li></ul></li><li><span><a href=\"#Visualization-with-Matplotlib\" data-toc-modified-id=\"Visualization-with-Matplotlib-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Visualization with Matplotlib</a></span><ul class=\"toc-item\"><li><span><a href=\"#Line-plot\" data-toc-modified-id=\"Line-plot-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Line plot</a></span></li><li><span><a href=\"#Box-plot\" data-toc-modified-id=\"Box-plot-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Box plot</a></span></li><li><span><a href=\"#Histogram\" data-toc-modified-id=\"Histogram-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Histogram</a></span></li><li><span><a href=\"#Bar-plots\" data-toc-modified-id=\"Bar-plots-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Bar plots</a></span></li><li><span><a href=\"#Horizontal-bars\" data-toc-modified-id=\"Horizontal-bars-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>Horizontal bars</a></span></li><li><span><a href=\"#Density-plot\" data-toc-modified-id=\"Density-plot-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>Density plot</a></span></li><li><span><a href=\"#Scatter-plot\" data-toc-modified-id=\"Scatter-plot-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>Scatter plot</a></span></li><li><span><a href=\"#Pie-chart\" data-toc-modified-id=\"Pie-chart-9.8\"><span class=\"toc-item-num\">9.8&nbsp;&nbsp;</span>Pie chart</a></span></li><li><span><a href=\"#Exercises-(15-min)\" data-toc-modified-id=\"Exercises-(15-min)-9.9\"><span class=\"toc-item-num\">9.9&nbsp;&nbsp;</span>Exercises (15 min)</a></span></li></ul></li><li><span><a href=\"#Final-Challenge-(45-min)\" data-toc-modified-id=\"Final-Challenge-(45-min)-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Final Challenge (45 min)</a></span></li><li><span><a href=\"#The-community\" data-toc-modified-id=\"The-community-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>The community</a></span></li><li><span><a href=\"#Acknowledgement-and-sources\" data-toc-modified-id=\"Acknowledgement-and-sources-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Acknowledgement and sources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "![pandas](https://pandas.pydata.org/pandas-docs/stable/_static/pandas.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a Python library that provides extensive tools for **data analysis and data management**. Data scientists often work with data stored in table formats like .csv, .tsv, or .xlsx. Pandas makes it very convenient to load, process, and analyze such tabular data. In conjunction with Matplotlib and Seaborn, Pandas provides a wide range of opportunities for **visual analysis of tabular data**.\n",
    "\n",
    "The main data structures in Pandas are implemented with `Series` and `DataFrame` classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of Series instances. DataFrames are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances.\n",
    "\n",
    "Pandas is well suited for many different kinds of data:\n",
    "\n",
    "* Tabular data with heterogeneously-typed columns, as in a SQL table or Excel spreadsheet.\n",
    "* Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "* Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels.\n",
    "* Any other form of observational/statistical data sets. The data actually need not to be labeled for them to be placed into a Pandas data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start working with Pandas just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that Numpy is imported with an abbreviation `np`, the same de-facto standard applies with Pandas, the usual import statement for Pandas is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy and Pandas work great together, so it is also useful to import Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Dataframe\n",
    "\n",
    "A DataFrame object is a two-dimensional table of data with column and row indexes (something like a spread sheet). The columns are made up of Series objects.\n",
    "\n",
    "![image.png](../data/PandasDataframe.png)\n",
    "\n",
    "## Columns and Row index\n",
    "A **DataFrame** has two Indexes:\n",
    "* Typically, the column index (df.columns) is a list of strings (variable names) or, less commonly, integers.\n",
    "* Typically, the row index (df.index) might be:\n",
    "    * Integers - for case or row numbers;\n",
    "    * Strings – for case names; or\n",
    "    * DatetimeIndex or PeriodIndex – for time series\n",
    "\n",
    "## Series objects\n",
    "**Series object**: an ordered, one-dimensional array of data with an index. All the data in a Series is of the same data type. Series arithmetic is vectorised after first aligning the Series index for each of the operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series acts very similarly to a ndarray, and is \n",
    "# a valid argument to most NumPy functions.\n",
    "s1 = pd.Series(np.arange(0,1)) # -> 0, 1, 2, 3 \n",
    "s2 = pd.Series(np.arange(1,5)) # -> 1, 2, 3, 4\n",
    "s3 = s1 + s2 # -> 1, 3, 5, 7\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Dataframe starting from dictionary\n",
    "D = {\n",
    "    \"one\" : np.arange(0,10),\n",
    "    \"two\" : np.arange(10,20),\n",
    "    \"three\" : np.arange(20,30),\n",
    "}\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstDF = pd.DataFrame(D) # Create a DataFrame starting from a Python dict\n",
    "myfirstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstDF.dtypes # Shows for every Series in the dataframe its dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstDF.shape # As we saw in Numpy, dataframes have shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstDF.one.shape # So the series have shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstDF.describe() # Show basic statistic for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(myfirstDF.describe()) # The description itself is a Dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Dataframe starting from the following arrays\n",
    "names = ['Bob','Jessica','Mary','John','Mel']\n",
    "births = np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(names, births)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the columns name\n",
    "df = pd.DataFrame(list(zip(names,births)), columns=[\"Name\",\"Births\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types comparison\n",
    "Here are some examples of datatypes across different libraries\n",
    "\n",
    "\n",
    "| Pandas dtype  \t| Python type \t| NumPy type                                                     \t| Usage                             \t|\n",
    "|---------------\t|-------------\t|----------------------------------------------------------------\t|-----------------------------------\t|\n",
    "| object        \t| str         \t| string_, unicode_                                              \t| Text                              \t|\n",
    "| int64         \t| int         \t| int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 \t| Integer numbers                   \t|\n",
    "| float64       \t| float       \t| float_, float16, float32, float64                              \t| Floating point numbers            \t|\n",
    "| bool          \t| bool        \t| bool_                                                          \t| True/False values                 \t|\n",
    "| datetime64    \t| NA          \t| datetime64[ns]                                                 \t| Date and time values              \t|\n",
    "| timedelta[ns] \t| NA          \t| NA                                                             \t| Differences between two datetimes \t|\n",
    "| category      \t| NA          \t| NA                                                             \t| Finite list of text values        \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/file.csv')    # Often works \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/file.csv', \n",
    "                 header=0,           # Define the first row as 'row index'\n",
    "                 index_col=0,        # Define the first column as 'column index'\n",
    "                 quotechar='\"',      # Quote character\n",
    "                 sep=';',            # Separator\n",
    "                 na_values = ['na', '-', '.', '']) # NULL values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from MS Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"../data/file.xlsx\") # Reads in the most \"basic\" way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.ExcelFile(\"../data/file.xlsx\") # Custom file descriptor for Excel formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.sheet_names  # Access the sheets' names within the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_sas('../data/airline.sas7bdat') # SAS taken from http://www.principlesofeconometrics.com/sas.htm\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read from a DB we need a tool which enables the communication between Python and external DBs.\n",
    "\n",
    "SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.\n",
    "\n",
    "It provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('tracks.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to DB\n",
    "engine.execute(\"DROP TABLE IF EXISTS tracks_backup\")\n",
    "df.to_sql(\"tracks_backup\",engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to dict\n",
    "d = df.to_dict()\n",
    "list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To string\n",
    "s = df.to_string()\n",
    "print(s.split('\\n')[0])\n",
    "print(s.split('\\n')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To numpy matrix\n",
    "np_array = df.values\n",
    "np_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a concise summary of a DataFrame.\n",
    "# This method prints information about a DataFrame including\n",
    "# the index dtype and column dtypes, non-null values and memory usage.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first N rows (by default N=5)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last N rows (by default N=5)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats cols (the default mode shows only the numerical columns)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage() # Show memory usage in bytes for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage().sum() # Show total memory usage in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy() # copy a DataFrame\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by Columns\n",
    "col=\"Composer\"\n",
    "df.sort_values(by=col).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Original df is not changed, see \"inplace\" argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2 = \"Milliseconds\"\n",
    "df.sort_values(by=[col, col2]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=[col, col2])\n",
    "# Sort the dataframe by index\n",
    "df_sorted.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possibile to sort over the column index \n",
    "df.sort_index(axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T # transpose rows and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.axes # list row and col indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(r_idx, c_idx) = df.axes # from above\n",
    "r_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # Series column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.empty  # True for empty DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ndim   # number of axes (it is 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape  # (row-count, column-count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size   # row-count * column-count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values # get a numpy array for df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.iteritems()) # iteritems returns an iterator over the columns of type generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the columns\n",
    "generator = df.iteritems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (column_name, series) in enumerate(generator):\n",
    "    print(f'Column is {column_name}\\nand the head values are\\n{series.head()}')\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the rows\n",
    "generator = df.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (row_index, series) in enumerate(generator):\n",
    "    print(f'Row index is {row_index}\\nand the head values are\\n{series.head()}')\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10) # Instead of using with statement, set once for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute value of numeric value\n",
    "df[\"Bytes\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations works \n",
    "df[\"Name\"].add(\", Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Name\"] + \", Title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Bytes\"] / (1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values for each column\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative max value\n",
    "mask = [\"Milliseconds\", \"Bytes\"] # Work only on this columns\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask].cummax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative min value\n",
    "df[mask].cummin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum\n",
    "df[mask].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the difference of a DataFrame element compared with another\n",
    "# element in the DataFrame (default is the element in the same column\n",
    "# of the previous row).\n",
    "df[mask].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "df[mask].dot([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select/filter rows/cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Dataframe by columns\n",
    "df.filter(items=['Name', 'Composer'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already use it!\n",
    "df[['Name', 'Composer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Dataframe by row  \n",
    "df.filter(items=[5,10], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns that contain specif string\n",
    "df.filter(like='Id')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on index\n",
    "mask = df.index.map(lambda x: not x%2)\n",
    "df.loc[mask] # get even rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on condition\n",
    "mask = df[\"Bytes\"]>50000000\n",
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access the Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column-wise operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.columns # get columns index\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = idx[0] # first column label\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = idx.tolist() # list of col labels \n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(idx)) # idx is Index type\n",
    "a = idx.values # nd-array of col labels\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Name':'Title','UnitPrice':'UnitPriceEuro'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the columns index name (in place)\n",
    "df.columns = ['Track', 'Title', 'Album', 'Media', 'Genere', 'Composer', \"Milliseconds\", \"B\", \"Price\"] \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollback !\n",
    "df.columns = ['TrackId', 'Name', 'AlbumId', 'MediaTypeId', 'GenreId', 'Composer', \"Milliseconds\", \"Bytes\", \"UnitPriceEuro\"] \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'] # select column and get a Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Name']] # select columns and get a Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TrackId','Name']]  # select 2-plus columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Name', 'TrackId']] # you can define the order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[1]] # select column by index position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[[0, 3, 4]]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[:-1]] # all except last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Composer = df.pop('Composer')  \n",
    "Composer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composer is not in df anymore\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put Composer back in\n",
    "df[\"Composer\"] = Composer\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access column by Attribute\n",
    "df.Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new columns to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MBytes'] = df.Bytes/(1024**2) # Create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised arithmetic on columns\n",
    "df[\"UnitPriceUSD\"] = df[\"UnitPriceEuro\"] / 1.14\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column \n",
    "df.drop(\"UnitPriceUSD\", axis=1) # Returns the dataframe without the dropped column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # The original df still have UnitPriceUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Composer', 'MBytes'],axis=1).head() # You can drop multiple cols at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"UnitPriceUSD\", axis=1, inplace=True) # With inplace=True the operation happens on the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column values set based on criteria\n",
    "df['NewUnitPriceEuro'] = df['UnitPriceEuro'].where(df['UnitPriceEuro']!=0.99, other=0.90)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common column element wise methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of unique values for the columns \n",
    "df.Composer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return an array with same shape of the dataframe index where each value say if the element is a NaN value or not.\n",
    "df['Composer'].isnull() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Composer'].notnull() # not isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Composer'].fillna(\"Empty\") # replace NaN w 0 s = df['col'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type conversions\n",
    "df['MBytes'].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Series is monotonically increasing\n",
    "df.TrackId.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Bytes.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Series is monotonically decreasing\n",
    "np.invert(df.TrackId).is_monotonic_decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bytes'].rolling(window=4, min_periods=4, center=False).mean() # Compute average on moving window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rowwise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.index # get row index\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0] # first row label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[-1] # last row label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.index.tolist() # get index as a list \n",
    "l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.values # get index as an nd-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a specific column as row index\n",
    "df.set_index('TrackId') # Returns a dataframe or see inplace as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultiIndex \n",
    "df = df.set_index(['TrackId', 'AlbumId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and create a new one\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename row index, element wise\n",
    "df.rename(index=lambda idx: \"row{}\".format(idx), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append rows to a dataframe\n",
    "df_to_append = pd.DataFrame(\n",
    "    {\"TrackId\":3505, \n",
    "     \"AlbumId\":42 ,\n",
    "     \"Name\":\"Under Pressure\", \n",
    "     \"MediaTypeId\":2, \n",
    "     \"GenreId\": 3,\n",
    "     \"Milliseconds\":375418, \n",
    "     \"UnitPriceEuro\":1.99,\n",
    "     \"NewUnitPriceEuro\":1.99,\n",
    "     \"Bytes\": 6290521,\n",
    "     \"MBytes\": 5.999108 ,\n",
    "     \"Composer\": \"David Bowie , John Richard Deacon , Brian Harold May , Freddie Mercury ,Roger Meddows Taylor\" \n",
    "    },\n",
    "    index=[\"row3504\"])\n",
    "df_to_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_to_append, sort=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row by index\n",
    "df = df.drop(\"row3504\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate\n",
    "df.drop_duplicates(\"TrackId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean row selection by condition on values in a column\n",
    "df[df['UnitPriceEuro'] > 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['UnitPriceEuro'] > 0.99) & (df['MBytes'] > 500)] # Combining boolean conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['GenreId'].isin([20,21])] # Verify if the element of the column is part of a list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['GenreId'].isin([20,21])]  # Not is in \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a column string type,contains specific string \n",
    "df.Composer = df.Composer.fillna(\"\") # We need to clean the column to work on be\n",
    "df[df['Composer'].str.contains('Bowie')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows by index \n",
    "mask = df[df['GenreId'].isin([20,21])].index\n",
    "display(mask)\n",
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a slice of rows by integer position\n",
    "df[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get integer position of rows that meet condition\n",
    "idx = np.where(df['GenreId'].isin([20,21])) # Numpy array, not a Series!\n",
    "print(idx)\n",
    "df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the np where to define a new Series, remember it is a element wise operation \n",
    "df['NewUnitPriceEuro'] = np.where(df['MBytes']>=500, 2.99, df['NewUnitPriceEuro'])\n",
    "df[df['MBytes']>=500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  Dataframe indexing\n",
    "We already used until now but let's summarize the different ways to access the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['row2846':'row2919'] # label slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3:7]  # integer slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[7:3]  # why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.UnitPriceEuro >= 1.99]  # Boolean Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"row2819\"] # select by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[\"row2819\",\"row2820\"]] # lable list/Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['row2819':'row2946'] # inclusive slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.UnitPriceEuro >= 1.99] # Boolean Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10]  # single row access by integer position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[20,12,15]]  # int list/Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:35]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access single element by label\n",
    "df.at[\"row30\", \"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access single element by index\n",
    "df.iat[30,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises (30 min)\n",
    "Open the notebook called \"Pandas Execises 1\" from the folder exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge join, concatenate\n",
    "\n",
    "Pandas provides various functionalities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                     index=[4, 5, 6, 7])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                    'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                    'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                    'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                    index=[8, 9, 10, 11])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat \n",
    "pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat and add a a multi-index for each df \n",
    "pd.concat([df1,df2,df3], keys=[\"x\", \"y\", \"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat on column axis  (with outer join)\n",
    "df2 = pd.DataFrame({'E': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'F': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'G': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    },\n",
    "                     index=[1, 2, 3 ,4]) # Watch out!\n",
    "display(df2)\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat on column axis  (with inner join)\n",
    "df2 = pd.DataFrame({'E': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'F': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'G': ['C4', 'C5', 'C6', 'C7']},\n",
    "                     index=[1, 2, 3 ,4])\n",
    "pd.concat([df1,df2], axis=1, join=\"inner\") # Only common rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat ignoring the index\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                     index=[1, 2, 3 ,4])\n",
    "pd.concat([df1,df2], ignore_index=True, sort=False)\n",
    "# As you can see the dataframe df2 have a new set of index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database-style DataFrame joining/merging\n",
    "\n",
    "Pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like base::merge.data.frame in R). The reason for this is careful algorithmic design and the internal layout of the data in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],\n",
    "                    'value': np.random.randn(4)})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'key': ['B', 'D', 'D', 'E'],\n",
    "                    'value': np.random.randn(4)})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Merge method perform a database-style join operation by one or multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "pd.merge(df1, df2, on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left outer join\n",
    "pd.merge(df1, df2, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right outer join \n",
    "pd.merge(df1, df2, on='key', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Join\n",
    "pd.merge(df1, df2, on='key', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join method it usually used to merge two dataframe on their indexed by default, if you need to merge on column that are not indexed, it is beter to use merge method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.set_index(\"key\", inplace=True)\n",
    "df2 # Notice that key is not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the suffix to avoid overlapping column index\n",
    "df1.join(df2, on=\"key\", how='outer', lsuffix=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby : split-apply-combine\n",
    "\n",
    "By “group by” we are referring to a process involving one or more of the following steps:\n",
    "\n",
    "* Splitting the data into groups based on some criteria.\n",
    "* Applying a function to each group independently.\n",
    "* Combining the results into a data structure.\n",
    "\n",
    "Out of these, the split step is the most straightforward. In fact, in many situations we may wish to split the data set into groups and do something with those groups. In the apply step, we might wish to one of the following:\n",
    "\n",
    "* Aggregation: compute a summary statistic (or statistics) for each group. Some examples:\n",
    "\n",
    "    * Compute group sums or means.\n",
    "    * Compute group sizes / counts.\n",
    "\n",
    "* Transformation: perform some group-specific computations and return a like-indexed object. Some examples:\n",
    "\n",
    "    * Standardize data (zscore) within a group.\n",
    "    * Filling NAs within groups with a value derived from each group.\n",
    "\n",
    "* Filtration: discard some groups, according to a group-wise computation that evaluates True or False. Some examples:\n",
    "\n",
    "    * Discard data that belongs to groups with only a few members.\n",
    "    * Filter out data based on the group sum or mean.\n",
    "\n",
    "* Some combination of the above: GroupBy will examine the results of the apply step and try to return a sensibly combined result if it doesn’t fit into either of the above two categories.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting an object into groups\n",
    "grouped = df.groupby(\"UnitPriceEuro\")\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate: get the unique count of each column\n",
    "grouped.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different operation of aggreation for each columns\n",
    "grouped.agg(\n",
    "    {\"GenreId\": pd.Series.nunique,\n",
    "    \"MBytes\": pd.Series.sum}     \n",
    ")\n",
    "# Note : The result dataframe is sorted by default in case you have big dataframe to aggregate, it is possible to disable the sort in order to improve the perfomance.\n",
    "# Note2: If there are any NaN or NaT values in the grouping key, these will be automatically excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the different groups \n",
    "grouped.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific group by index\n",
    "grouped.get_group(1.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation functions\n",
    "\n",
    "| Function   \t| Description                                \t|\n",
    "|------------\t|--------------------------------------------\t|\n",
    "| mean()     \t| Compute mean of groups                     \t|\n",
    "| sum()      \t| Compute sum of group values                \t|\n",
    "| size()     \t| Compute group sizes                        \t|\n",
    "| count()    \t| Compute count of group                     \t|\n",
    "| std()      \t| Standard deviation of groups               \t|\n",
    "| var()      \t| Compute variance of groups                 \t|\n",
    "| sem()      \t| Standard error of the mean of groups       \t|\n",
    "| describe() \t| Generates descriptive statistics           \t|\n",
    "| first()    \t| Compute first of group values              \t|\n",
    "| last()     \t| Compute last of group values               \t|\n",
    "| nth()      \t| Take nth value, or a subset if n is a list \t|\n",
    "| min()      \t| Compute min of group values                \t|\n",
    "| max()      \t| Compute max of group values                \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traformation: returns an object that is indexed the same (same size) as the one being grouped. \n",
    "grouped[\"MBytes\"].transform(lambda x: x.max() + x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.transform(lambda x: x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why NewUnitPriceEuro is 0?\n",
    "df[df.UnitPriceEuro == 0.99].NewUnitPriceEuro.max(),df[df.UnitPriceEuro == 0.99].NewUnitPriceEuro.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtration: The filter method returns a subset of the original object. \n",
    "globalmean = df[\"MBytes\"].mean()\n",
    "grouped.filter(lambda x: x[\"MBytes\"].mean() > globalmean )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply**: Some operations on the grouped data might not fit into either the aggregate or transform categories. \n",
    "Or, you may simply want GroupBy to infer how to combine the results. \n",
    "\n",
    "For these, use the apply function, which can be substituted for both aggregate and transform in many standard use cases. \n",
    "\n",
    "**Apply** can act as a reducer, transformer, or filter function, depending on exactly what is passed to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped[\"MBytes\"].apply(lambda x: x.describe()).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Name\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping on Continuous Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Milliseconds.describe() # Consider the Milliseconds column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, M = df.Milliseconds.min(), df.Milliseconds.max()\n",
    "bucket_size = 60000\n",
    "df.groupby(pd.cut(df.Milliseconds, np.arange(m, M+bucket_size, bucket_size))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with missing data\n",
    "Pandas uses the not-a-number construct (np.nan and float('nan')) to indicate missing data. \n",
    "The Python None can arise in data as well. \n",
    "It is also treated as missing data; as is the pandas not-a-time construct (pandas.NaT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([8,None,float('nan'),np.nan]) #[8, NaN, NaN, NaN]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Series where check element wise if is NaN value\n",
    "s.isnull() #[False, True, True, True] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Series where check element wise if is not a NaN value\n",
    "s.notnull() #[True, False, False, False] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values 0\n",
    "s.fillna(0) #[8, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = pd.DataFrame(np.random.randn(5, 3), \n",
    "                      index=['a', 'c', 'e', 'f', 'h'],\n",
    "                      columns=['one', 'two', 'three'])\n",
    "df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = df_nan.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) # reindex might insert NaNs\n",
    "df_nan.iloc[0,0] = None\n",
    "df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan.dropna()          # drop all rows with NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan.dropna(how='all') # drop all row where all the values are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan[\"three\"].fillna(0, inplace=True)\n",
    "df_nan.dropna(axis=1)    # drop all columns with NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only drop a row if NaN in a specified col \n",
    "df_nan.dropna(axis=0, subset=[\"two\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises (15 min)\n",
    "Open the notebook called \"Pandas Exercises 2\" inside exercises folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot\n",
    "![image.png](../data/reshaping_pivot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "data = u\"\"\"Data,Agenzia,Regione,Partito,Stima\n",
    "13/03/2014, Opinio, Puglia, PD, 25\n",
    "13/03/2014, Opinio, Puglia, FI, 28\n",
    "13/03/2014, Opinio, Lombardia, PD, 24 \n",
    "13/03/2014, Opinio, Lombardia, FI, 23\n",
    "13/03/2014, EMG, Puglia, PD, 23\n",
    "13/03/2014, EMG, Puglia, FI, 24\n",
    "13/03/2014, EMG, Lombardia, PD, 26\n",
    "13/03/2014, EMG, Lombardia, FI, 25\n",
    "13/03/2014, EMG, Veneto, PD, 21\n",
    "13/03/2014, EMG, Veneto, FI, 27\"\"\" \n",
    "\n",
    "df_poll = pd.read_csv(StringIO(data), header=0, skipinitialspace=True)\n",
    "df_poll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot to wide format on 'Partito' column \n",
    "# 1st: set up a MultiIndex for other cols \n",
    "df1 = df_poll.set_index(['Data', 'Agenzia', 'Regione'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poll.pivot(columns='Partito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd: do the pivot\n",
    "wide1 = df1.pivot(columns='Partito')\n",
    "wide1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack / Unstack\n",
    "\n",
    "**Stack**:“pivot” a level of the (possibly hierarchical) column labels, returning a DataFrame with an index with a new inner-most level of row labels.\n",
    "![image.png](../data/reshaping_stack.png)\n",
    "\n",
    "**Unstack**: (inverse operation of stack) “pivot” a level of the (possibly hierarchical) row index to the column axis, producing a reshaped DataFrame with a new inner-most level of column labels.\n",
    "\n",
    "![image.png](../data/reshaping_unstack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack to wide format on Regione / Partito \n",
    "# 1st: MultiIndex all but the Values col \n",
    "df2 = df_poll.set_index(['Data', 'Agenzia', 'Regione', 'Partito'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd: unstack a column to go wide on it \n",
    "wide2 = df2.unstack('Regione')\n",
    "wide2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide3 = df2.unstack() # default : pop last index\n",
    "wide3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stack() to get back to long format \n",
    "long1 = wide1.stack()\n",
    "long1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use reset_index() to remove the\n",
    "# MultiIndex.\n",
    "long2 = long1.reset_index()\n",
    "long2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melt\n",
    "\n",
    "\"Unpivots\" a DataFrame from wide format to long format.\n",
    "\n",
    "![image.png](../data/reshaping_melt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt() back to long format\n",
    "# 1st: flatten the column index \n",
    "wide1.columns = ['_'.join(col) for col in wide1.columns.values] \n",
    "print(wide1.columns)\n",
    "wide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd: remove the MultiIndex\n",
    "wide1 = wide1.reset_index()\n",
    "wide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd: melt away\n",
    "long3 = pd.melt(wide1, \n",
    "                value_vars=['Stima_FI', 'Stima_PD'], \n",
    "                var_name='Partito', \n",
    "                id_vars=['Data', 'Agenzia', 'Regione']\n",
    "               )\n",
    "long3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstab\n",
    "Use crosstab() to compute a cross-tabulation of two (or more) factors. \n",
    "\n",
    "By default crosstab computes a frequency table of the factors unless an array of values and an aggregation function are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_poll.Agenzia, df_poll.Regione, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator / dummy variables\n",
    "\n",
    "To convert a categorical variable into a “dummy” or “indicator” DataFrame: for example a column in a DataFrame (a Series) which has k distinct values, can derive a DataFrame containing k columns of 1s and 0s using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df_poll[['Agenzia', 'Regione']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with timeseries\n",
    "\n",
    "Pandas has proven very successful as a tool for working with time series data, especially in the financial data analysis space.\n",
    "\n",
    "Working with time series data, we will frequently seek to:\n",
    "\n",
    "* generate sequences of fixed-frequency dates and time spans\n",
    "* conform or convert time series to a particular frequency\n",
    "* compute “relative” dates based on various non-standard time increments (e.g. 5 business days before the last business day of the year), or “roll” dates forward or backward\n",
    "\n",
    "pandas provides a relatively compact and self-contained set of tools for performing the above tasks.\n",
    "\n",
    "## Timestamp vs Span\n",
    "\n",
    "With its focus on time-series data, pandas has a suite of tools for managing dates and time: either as a point in time (a Timestamp) or as a span of time (a Period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp('2013-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp('2013-01-01 21:15:06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp('2013-01-01 21:15:06.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Period('2013-01-01', freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ['2015-04-01', '2014-04-02']\n",
    "# Series of Timestamps (good)\n",
    "s = pd.to_datetime(pd.Series(ts))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series of Periods (in months) \n",
    "pd.Series([pd.Period(x, freq='M') for x in ts] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period is useful as index\n",
    "pd.Series(pd.PeriodIndex(ts,freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pd.PeriodIndex(ts,freq='Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['09:08:55.7654-JAN092002', '15:42:02.6589-FEB082016']\n",
    "pd.Series(pd.to_datetime(t, format=\"%H:%M:%S.%f-%b%d%Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatetimeIndex and Periodindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_strs = ['2014-01-01', '2014-04-01', '2014-07-01', '2014-10-01']\n",
    "dti = pd.DatetimeIndex(date_strs)\n",
    "dti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = pd.PeriodIndex(date_strs, freq='D') \n",
    "pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim = pd.PeriodIndex(date_strs, freq='M') \n",
    "pim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piq = pd.PeriodIndex(date_strs, freq='Q')\n",
    "piq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pid[1] - pid[0]) # 90 days \n",
    "print (pim[1] - pim[0]) # 3 months \n",
    "print (piq[1] - piq[0]) # 1 quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_strs = ['2015-01-01 02:10:40.12345', '2015-01-01 02:10:50.67890'] \n",
    "pis = pd.PeriodIndex(time_strs, freq='U') # Microseconds frequency\n",
    "pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Period index to an existing dataframe\n",
    "df.index = pd.period_range('2015-01', periods=len(df), freq='D')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti = pd.to_datetime(['04-01-2012'], dayfirst=True) # USA date format\n",
    "dti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a period index from a range of dates\n",
    "pi = pd.period_range('1960-01-01', '2015-12-31', freq='Q')\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pd.period_range('1960-01-01', '2015-12-31', freq='H')\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the frequency to days\n",
    "pi.asfreq(freq=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Pandas Period string*\n",
    "![image.png](../data/Pandas-Period-String.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling frequency\n",
    "resample is similar to using a rolling() operation with a time-based offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample(\"A\")[\"MBytes\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row selection with timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "february_selector = (df.index.month == 2)\n",
    "february_data = df[february_selector]\n",
    "february_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Q1 data\n",
    "df[(df.index.month >= 1) & (df.index.month <= 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.index.year)[\"Bytes\", \"Milliseconds\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises (20 min)\n",
    "\n",
    "Open the notebook called \"Pandas Execises 3\" from the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable the plot in your jupyter you have first to run\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Matplotlib\n",
    "## Line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "cols = [\"MBytes\"]\n",
    "df1 = df[cols].cumsum()\n",
    "ax = df1.plot()\n",
    "ax.set_title(\"Storage weight\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"MByte\")\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.figure\n",
    "fig.set_size_inches(8, 3) \n",
    "fig.tight_layout(pad=1) \n",
    "fig.savefig('filename.png', dpi=125)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Minutes\"] = df[\"Milliseconds\"] / 60000\n",
    "df[\"Minutes\"].plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MBytes\"].plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MBytes\"].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Minutes\"].plot.hist(bins=20, xlim=(50, 100), ylim=(0,10), title='Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new sample dataframe \n",
    "np.random.seed(42)\n",
    "a = np.random.normal(0,1,999) \n",
    "b = np.random.normal(1,2,999)\n",
    "c = np.random.normal(2,3,999) \n",
    "df_sample = pd.DataFrame([a,b,c]).T \n",
    "df_sample.columns = ['A', 'B', 'C']\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-10,10,21) \n",
    "binned = pd.DataFrame()\n",
    "for x in df_sample.columns:\n",
    "    y=pd.cut(df_sample[x],bins,labels=bins[:-1]) \n",
    "    y=y.value_counts().sort_index() \n",
    "    binned = pd.concat([binned,y],axis=1)\n",
    "binned.index = binned.index.astype(float) \n",
    "binned.index += (np.diff(bins) / 2.0)\n",
    "binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = binned.plot.bar(stacked=False, width=0.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = binned['A'][(binned.index >= -4) & (binned.index <= 4)].plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.plot.scatter(x='A', y='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_group = df.groupby(\"GenreId\").agg({\"TrackId\":\"count\"})\n",
    "genre_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopct_generator(limit):\n",
    "    \"\"\"Remove percent on small slices.\"\"\"\n",
    "    def inner_autopct(pct):\n",
    "        return ('%.2f%%' % pct) if pct > limit else ''\n",
    "    return inner_autopct\n",
    "\n",
    "genre_group.plot.pie(\n",
    "    figsize=(12,13),y=\"TrackId\", \n",
    "    autopct=autopct_generator(7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises (15 min)\n",
    "\n",
    "Open the notebook called \"Pandas Execises 4\" from the exercises folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Challenge (45 min)\n",
    "\n",
    "Open the notebook called \"Pandas Challenge\" from the exercises folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Pandas official website](https://pandas.pydata.org/)\n",
    "- [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/): official documentation for Pandas\n",
    "- [Pandas getting started](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html)\n",
    "- [Pandas vs SAS](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sas.html)\n",
    "- [Pandas vs SQL](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)\n",
    "- [Pandas indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\n",
    "- [Pandas merge and join](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    "- [Pandas profiling](https://github.com/pandas-profiling/pandas-profiling): third party library to create reports\n",
    "- [Pydata](https://pydata.org/): community of users and developers of data analysis tools\n",
    "- [stackoverflow/pandas](http://stackoverflow.com/questions/tagged/pandas): where almost every developer learns, shares and builds her programming skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement and sources\n",
    "- [Pandas notes](http://markthegraph.blogspot.com/2014/12/updated-cheat-sheets-python-pandas-and.html) some usefull notes for Pandas - Matplotlib\n",
    "- [Tutorial for Pandas Analisys](https://mlcourse.ai/notebooks/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis.ipynb?flush_cache=true) Great resource , with a lot of tutorial and example to work with Pandas\n",
    "- [Pandas exercises](https://github.com/guipsamora/pandas_exercises ) Useful repository for practice with Pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
