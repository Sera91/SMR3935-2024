{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp7y0Y26ChhJ"
   },
   "source": [
    "# GPU programming in Python\n",
    "\n",
    "## Short refresh on technical vocabulary\n",
    "\n",
    "Several important terms in the topic of CUDA programming are listed here:\n",
    "-  host: the CPU\n",
    "-  device: the GPU\n",
    "-  host memory: the system main memory\n",
    "-  device memory: onboard memory on a GPU card\n",
    "-  kernel: a GPU function launched by the host and executed on the device\n",
    "-  device function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n",
    "\n",
    "\n",
    "The typycal GPU structure is illustrated below\n",
    "\n",
    "<img src=\"images/CUDA-threads.png\" width=\"75%\" height=\"75%\" />\n",
    "\n",
    "## Cupy\n",
    "\n",
    "<img src=\"images/cupy.png\" width=\"75%\" height=\"75%\" />\n",
    "\n",
    "CuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python. CuPy acts as a drop-in replacement to run existing NumPy/SciPy code on NVIDIA CUDA or AMD ROCm platforms.\n",
    "\n",
    "\n",
    "### Installation\n",
    "\n",
    "Before installing Cupy is recommended to install setuptools\n",
    "\n",
    "```\n",
    "$ python -m pip install -U setuptools pip\n",
    "```\n",
    "\n",
    "Then we can install cupy with conda, for a specific cuda version as\n",
    "\n",
    "```\n",
    "$ conda install -c conda-forge cupy cuda-version=11.8\n",
    "```\n",
    "\n",
    "I suggest to install also CUDNN and CUTENSOR\n",
    "```\n",
    "$ conda install -c conda-forge cupy cudnn cutensor nccl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2INX1mPAI1On"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LdgOVqdIkWR"
   },
   "source": [
    "Let's create some arrays on CPU and GPU and compare time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Os1pHT9bCfYv",
    "outputId": "9ad34522-b493-416b-881f-ec7c020a8284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 ms ± 2.73 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "#CPU\n",
    "x_on_cpu = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "%timeit -n 100 x_cpu = np.ones((100,500,500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlC6rJ1JI6qd",
    "outputId": "9d4bf191-ef0b-4419-9eff-1bf643f4ef82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 µs ± 13 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "#GPU\n",
    "x_on_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "%timeit -n 100 x_gpu = cp.ones((100,500,500))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGfLDt8CI-0Z"
   },
   "source": [
    "**Data transfer**\n",
    "\n",
    "*How do we move arrays from CPU to GPUs?*\n",
    "\n",
    "In CuPy, all CUDA operations such as data transfer (see the Data Transfer section) and kernel launches are enqueued onto the current stream, and the queued tasks on the same stream will be executed in serial (but asynchronously with respect to the host).\n",
    "\n",
    "The default current stream in CuPy is CUDA’s null stream (i.e., stream 0). It is also known as the legacy default stream, which is unique per device. However, it is possible to change the current stream using the cupy.cuda.Stream API, please see Accessing CUDA Functionalities for example. The current stream in CuPy can be retrieved using cupy.cuda.get_current_stream().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jn99Ew7uI9_T",
    "outputId": "108d2cef-006f-4fde-a371-d8ea594cc1bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7 µs ± 943 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "18.5 µs ± 2.24 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "17.5 µs ± 2.14 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# FROM HOST TO DEVICE\n",
    "\n",
    "x_cpu = np.array([1, 2, 3])\n",
    "%timeit x_gpu = cp.asarray(x_cpu)  # move the data to the current device.\n",
    "\n",
    "#VICEVERSA: FROM DEVICE TO HOST\n",
    "\n",
    "x_gpu = cp.array([1, 2, 3])  # create an array in the current device\n",
    "%timeit x_cpu = cp.asnumpy(x_gpu)  # move the array to the host.\n",
    "\n",
    "#alternative:\n",
    "%timeit x_cpu = x_gpu.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jj5DYNXL56V"
   },
   "source": [
    "using the Device context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D39hPXnVL9nz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "  x_on_gpu0 = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "#with cp.cuda.Device(1):\n",
    "#   x_on_gpu1 = cp.array([1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BkWoDqLOiuD"
   },
   "source": [
    "## How to write CPU/GPU agnostic code\n",
    "CuPy’s compatibility with NumPy makes it possible to write CPU/GPU agnostic code. For this purpose, CuPy implements the cupy.get_array_module() function that returns a reference to cupy if any of its arguments resides on a GPU and numpy otherwise. Here is an example of a CPU/GPU agnostic function that computes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-gc602AOmCP"
   },
   "outputs": [],
   "source": [
    "# Stable implementation of log(1 + exp(x))\n",
    "def softplus(x):\n",
    "    xp = cp.get_array_module(x)  # 'xp' is a standard usage in the community\n",
    "    print(\"Using:\", xp.__name__)\n",
    "    return xp.maximum(0, x) + xp.log1p(xp.exp(-abs(x)))\n",
    "\n",
    "x_cpu = np.random.random(1000)\n",
    "\n",
    "%timeit softplus(x_cpu)\n",
    "\n",
    "x_gpu = cp.random.random(1000)\n",
    "\n",
    "%timeit softplus(x_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z3YIMdjIkOH"
   },
   "source": [
    "Let's compare the execution time on CPU and GPUs of 3 different linear algebra operations:\n",
    "- matmul\n",
    "- SVD (Singular Value Decomposition)\n",
    "- trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zUm0UhAJiHK",
    "outputId": "3017bc6b-7851-4e6a-ab72-555669542b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "8.4 µs ± 4.39 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Matmul on CPU\n",
    "a = np.random.rand(1000,1000)\n",
    "b = np.random.rand(1000,1000)\n",
    "\n",
    "%timeit -n 10 np.matmul(a,b,out=None)\n",
    "\n",
    "%timeit -n 10 np.trace(a, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlfwWV-sJk8x",
    "outputId": "f7be265f-6a37-4ebb-c587-ecddb31ae0f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "349 µs ± 250 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "66.3 µs ± 7.05 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Matmul on GPU\n",
    "a = cp.random.rand(1000,1000)\n",
    "b = cp.random.rand(1000,1000)\n",
    "\n",
    "%timeit -n 10 cp.matmul(a,b,out=None)\n",
    "\n",
    "\n",
    "%timeit -n 10 cp.trace(a, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is-tduSqIjtR",
    "outputId": "539cc829-365e-4939-92c7-3b776a4ed6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 s, sys: 285 ms, total: 1.66 s\n",
      "Wall time: 895 ms\n"
     ]
    }
   ],
   "source": [
    "#SVD on CPU\n",
    "%%time\n",
    "x_cpu = np.random.random((1000, 1000))\n",
    "u, s, v = np.linalg.svd(x_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWKsyZ96JsTI",
    "outputId": "31ad6c5b-54fc-4afc-eb3a-1e6fe5a17f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 316 ms, total: 1.89 s\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "#SVD on GPU\n",
    "\n",
    "%%time\n",
    "x_gpu = cp.random.random((1000, 1000))\n",
    "u, s, v = cp.linalg.svd(x_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYRRRKWIOdnQ"
   },
   "source": [
    "this is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Irq6y5NmT8v9",
    "outputId": "b577178b-ba77-42b2-e42d-664c74eedf68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time measured with cuda event 368.1689453125\n",
      "time measure with time 0.36806521299968153\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_gpu = cp.cuda.Event()\n",
    "end_gpu = cp.cuda.Event()\n",
    "\n",
    "start_gpu.record()\n",
    "start_cpu = time.perf_counter()\n",
    "x_gpu = cp.random.random((1000, 1000))\n",
    "u, s, v = cp.linalg.svd(x_gpu)\n",
    "end_cpu = time.perf_counter()\n",
    "end_gpu.record()\n",
    "end_gpu.synchronize()\n",
    "t_gpu = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "t_cpu = end_cpu - start_cpu\n",
    "\n",
    "print(\"time measured with cuda event\", t_gpu)\n",
    "print(\"time measure with time\", t_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YilEqMJYU0gH",
    "outputId": "8f52b3f4-3d20-4513-84ac-b017b74e98f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum                 :    CPU:   25.292 us   +/- 5.547 (min:   21.103 / max:   51.869) us     GPU-0:  101.437 us   +/- 5.273 (min:   96.256 / max:  125.952) us\n"
     ]
    }
   ],
   "source": [
    "#CUPY PROFILER\n",
    "from cupyx.profiler import benchmark\n",
    "a = cp.random.random((256, 256, 256), dtype=cp.float32)\n",
    "print(benchmark(a.sum, (), n_repeat=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnssmGe3Qsuz"
   },
   "source": [
    "## Memory management\n",
    "CuPy uses memory pool for memory allocations by default. The memory pool significantly improves the performance by mitigating the overhead of memory allocation and CPU/GPU synchronization.\n",
    "\n",
    "There are two different memory pools in CuPy:\n",
    "\n",
    "   - Device memory pool (GPU device memory), which is used for GPU memory allocations.\n",
    "\n",
    "   - Pinned memory pool (non-swappable CPU memory), which is used during CPU-to-GPU data transfer.\n",
    "\n",
    "\n",
    "You can clear the memory pool by calling `free_all_blocks`.\n",
    "\n",
    "```python\n",
    "mempool.free_all_blocks()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdc4zFsaQ8nv",
    "outputId": "60069339-6add-41b9-d94a-fb496298c159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "24009216\n",
      "6000000512\n",
      "1\n",
      "400\n",
      "24009728\n",
      "6000000512\n",
      "1\n",
      "24009216\n",
      "6000000512\n",
      "1\n",
      "24009216\n",
      "2000000512\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import cupy\n",
    "import numpy\n",
    "\n",
    "mempool = cupy.get_default_memory_pool()\n",
    "pinned_mempool = cupy.get_default_pinned_memory_pool()\n",
    "\n",
    "# Create an array on CPU.\n",
    "# NumPy allocates 400 bytes in CPU (not managed by CuPy memory pool).\n",
    "a_cpu = numpy.ndarray(100, dtype=numpy.float32)\n",
    "print(a_cpu.nbytes)                      # 400\n",
    "\n",
    "# You can access statistics of these memory pools.\n",
    "print(mempool.used_bytes())              # 0\n",
    "print(mempool.total_bytes())             # 0\n",
    "print(pinned_mempool.n_free_blocks())    # 0\n",
    "\n",
    "# Transfer the array from CPU to GPU.\n",
    "# This allocates 400 bytes from the device memory pool, and another 400\n",
    "# bytes from the pinned memory pool.  The allocated pinned memory will be\n",
    "# released just after the transfer is complete.  Note that the actual\n",
    "# allocation size may be rounded to larger value than the requested size\n",
    "# for performance.\n",
    "a = cupy.array(a_cpu)\n",
    "print(a.nbytes)                          # 400\n",
    "print(mempool.used_bytes())              # 512\n",
    "print(mempool.total_bytes())             # 512\n",
    "print(pinned_mempool.n_free_blocks())    # 1\n",
    "\n",
    "# When the array goes out of scope, the allocated device memory is released\n",
    "# and kept in the pool for future reuse.\n",
    "a = None  # (or `del a`)\n",
    "print(mempool.used_bytes())              # 0\n",
    "print(mempool.total_bytes())             # 512\n",
    "print(pinned_mempool.n_free_blocks())    # 1\n",
    "\n",
    "# clearing the memory pool by calling `free_all_blocks`.\n",
    "mempool.free_all_blocks()\n",
    "pinned_mempool.free_all_blocks()\n",
    "print(mempool.used_bytes())              # 0\n",
    "print(mempool.total_bytes())             # 0\n",
    "print(pinned_mempool.n_free_blocks())    # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzHuubjPRH6O"
   },
   "outputs": [],
   "source": [
    "# You can also specify the limit in fraction of the total amount of memory\n",
    "# on the GPU. If you have a GPU with 2 GiB memory, the following is\n",
    "# equivalent to the above configuration.\n",
    "#   $ export CUPY_GPU_MEMORY_LIMIT=\"50%\"\n",
    "\n",
    "import cupy\n",
    "print(cupy.get_default_memory_pool().get_limit())\n",
    "\n",
    "mempool = cupy.get_default_memory_pool()\n",
    "\n",
    "with cupy.cuda.Device(0):\n",
    "    mempool.set_limit(size=1024**3)  # 1 GiBmempool = cupy.get_default_memory_pool()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhuOTckiS5LP"
   },
   "source": [
    "### New feature: Stream Ordered memory allocator\n",
    "\n",
    " Similar to CuPy’s memory pool, Stream Ordered Memory Allocator also allocates/deallocates memory asynchronously from/to a memory pool in a stream-ordered fashion. The key difference is that it is a built-in feature implemented in the CUDA driver by NVIDIA, so other CUDA applications in the same processs can easily allocate memory from the same pool.\n",
    "\n",
    "To enable a memory pool that manages stream ordered memory, you can construct a new MemoryAsyncPool instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Np6sDrpmSjMX"
   },
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "# Use asynchronous stream ordered memory\n",
    "cupy.cuda.set_allocator(cupy.cuda.MemoryAsyncPool().malloc)\n",
    "\n",
    "# Create a custom stream\n",
    "s = cupy.cuda.Stream()\n",
    "\n",
    "# This would allocate memory asynchronously on stream s\n",
    "with s:\n",
    "    a = cupy.empty((100,), dtype=cupy.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mpy7qDsTIHl"
   },
   "source": [
    "Note that if you pass malloc_async() directly to set_allocator() without constructing a MemoryAsyncPool instance, the device’s current memory pool will be used.\n",
    "\n",
    "When using stream ordered memory, it is important that you maintain a correct stream semantics yourselves using, for example, the Stream and Event APIs (see Streams and Events for details); CuPy does not attempt to act smartly for you. Upon deallocation, the memory is freed asynchronously either on the stream it was allocated (first attempt), or on any current CuPy stream (second attempt). It is permitted that the stream on which the memory was allocated gets destroyed before all memory allocated on it is freed.\n",
    "\n",
    "In addition, applications/libraries internally use cudaMalloc (CUDA’s default, synchronous allocator) could have unexpected interplay with Stream Ordered Memory Allocator. Specifically, memory freed to the memory pool might not be immediately visible to cudaMalloc, leading to potential out-of-memory errors. In this case, you can either call free_all_blocks() or just manually perform a (event/stream/device) synchronization, and retry.\n",
    "\n",
    "Currently the MemoryAsyncPool interface is experimental. In particular, while its API is largely identical to that of MemoryPool, several of the pool’s methods require a sufficiently new driver (and of course, a supported hardware, CUDA version, and platform) due to CUDA’s limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvIEM4gxWw5x"
   },
   "source": [
    ". Data copies and kernel launches are enqueued onto the Current Stream, which can be queried via get_current_stream() and changed either by setting up a context manager or by using use().\n",
    "\n",
    "Events can be created either manually or through the record() method, applied to stream. Event objects can be used for timing GPU activities (via get_elapsed_time()) or setting up inter-stream dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4hXhlM6Wn6M"
   },
   "outputs": [],
   "source": [
    "#using context manager to switch stream\n",
    "import numpy as np\n",
    "\n",
    "a_np = np.arange(10)\n",
    "s = cp.cuda.Stream()\n",
    "with s:\n",
    "    a_cp = cp.asarray(a_np)  # H2D transfer on stream s\n",
    "    b_cp = cp.sum(a_cp)      # kernel launched on stream s\n",
    "    assert s == cp.cuda.get_current_stream()\n",
    "\n",
    "# fall back to the previous stream in use (here the default stream)\n",
    "# when going out of the scope of s\n",
    "\n",
    "\n",
    "#using use()\n",
    "\n",
    "s = cp.cuda.Stream()\n",
    "s.use()  # any subsequent operations are done on steam s\n",
    "\n",
    "b_np = cp.asnumpy(b_cp)\n",
    "assert s == cp.cuda.get_current_stream()\n",
    "cp.cuda.Stream.null.use()  # fall back to the default (null) stream\n",
    "\n",
    "assert cp.cuda.Stream.null == cp.cuda.get_current_stream()\n",
    "\n",
    "\n",
    "\n",
    "e1 = cp.cuda.Event()\n",
    "e1.record()\n",
    "a_cp = b_cp * a_cp + 8\n",
    "e2 = cp.cuda.get_current_stream().record()\n",
    "\n",
    "# set up a stream order\n",
    "s2 = cp.cuda.Stream()\n",
    "s2.wait_event(e2)\n",
    "with s2:\n",
    "    # the a_cp is guaranteed updated when this copy (on s2) starts\n",
    "    a_np = cp.asnumpy(a_cp)\n",
    "\n",
    "# timing\n",
    "e2.synchronize()\n",
    "t = cp.cuda.get_elapsed_time(e1, e2)  # only include the compute time, not the copy time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ensTzEC6YfyX"
   },
   "source": [
    "# User-Defined Kernels\n",
    "CuPy provides easy ways to define three types of CUDA kernels: elementwise kernels, reduction kernels and raw kernels. In this documentation, we describe how to define and call each kernels.\n",
    "\n",
    "There exist three main types of user defined kernels:\n",
    "- elementwise kernel\n",
    "- reduce kernel\n",
    "- raw kernel\n",
    "\n",
    "**Elementwise kernel**\n",
    "\n",
    "An elementwise kernel can be defined by the ElementwiseKernel class (cp.ElementwiseKernel()). The instance of this class defines a CUDA kernel which can be invoked by the __call__ method of this instance.\n",
    "\n",
    "A definition of an elementwise kernel consists of four parts:\n",
    "1. an input argument list (of fixed or generic type)\n",
    "2. an output argument list\n",
    "3. a loop body code\n",
    "4. the kernel name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3kEaEx9bbvx"
   },
   "outputs": [],
   "source": [
    "squared_diff = cp.ElementwiseKernel(\n",
    "   'float32 x, float32 y',\n",
    "   'float32 z',\n",
    "   'z = (x - y) * (x - y)',\n",
    "   'squared_diff')\n",
    "\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2, 5)\n",
    "y = cp.random.random(10, dtype=np.float32).reshape(2, 5)\n",
    "squared_diff(x,y)\n",
    "\n",
    "#the cupy kernel is able also to perform broadcasting\n",
    "y = cp.arange(5, dtype=np.float32)\n",
    "squared_diff(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2-zxdFMbcgS"
   },
   "source": [
    "### Type-generic kernels\n",
    "If a type specifier is one character, then it is treated as a type placeholder. It can be used to define a type-generic kernels.\n",
    "\n",
    "Type placeholders of a same character in the kernel definition indicate the same type. The actual type of these placeholders is determined by the actual argument type. The ElementwiseKernel class first checks the output arguments and then the input arguments to determine the actual type. If no output arguments are given on the kernel invocation, then only the input arguments are used to determine the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89Xx4HLsTiBi",
    "outputId": "6985b336-a108-4e5d-f617-fd48b3320c52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2708994e-01, 5.7552494e-03, 1.4336106e+00, 5.4559960e+00,\n",
       "        1.0588921e+01],\n",
       "       [1.9140984e+01, 2.8782030e+01, 4.7485634e+01, 5.7307125e+01,\n",
       "        6.4585457e+01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Elementwise kernel with different degree of generability\n",
    "\n",
    "squared_diff_generic = cp.ElementwiseKernel(\n",
    "    'T x, T y',\n",
    "    'T z',\n",
    "    'z = (x - y) * (x - y)',\n",
    "    'squared_diff_generic')\n",
    "\n",
    "#We can assume to have the same generic type for input and output\n",
    "#and we can put the type placeholders in the body of the kernel\n",
    "\n",
    "squared_diff_more_generic = cp.ElementwiseKernel(\n",
    "    'T x, T y',\n",
    "    'T z',\n",
    "    '''\n",
    "        T diff = x - y;\n",
    "        z = diff * diff;\n",
    "    ''',\n",
    "    'squared_diff_generic')\n",
    "\n",
    "#We can assume differen type for different input array and for the output\n",
    "\n",
    "squared_diff_super_generic = cp.ElementwiseKernel(\n",
    "    'X x, Y y',\n",
    "    'Z z',\n",
    "    'z = (x - y) * (x - y)',\n",
    "    'squared_diff_super_generic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH7SBsaAZQWE"
   },
   "source": [
    "**Reduce kernel**\n",
    "\n",
    "Reduction kernels can be defined by the ReductionKernel class. We can use it by defining four parts of the kernel code:\n",
    "\n",
    "1.  Identity value: This value is used for the initial value of reduction.\n",
    "\n",
    "2.  Mapping expression: It is used for the pre-processing of each element to be reduced.\n",
    "\n",
    "3.  Reduction expression: It is an operator to reduce the multiple mapped values. The special variables a and b are used for its operands.\n",
    "\n",
    "4.  Post mapping expression: It is used to transform the resulting reduced values. The special variable a is used as its input. Output should be written to the output parameter.\n",
    "\n",
    "ReductionKernel class automatically inserts other code fragments that are required for an efficient and flexible reduction implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t4aydVQZUbY"
   },
   "outputs": [],
   "source": [
    "#L2 norm along specified axes can be written as follows:\n",
    "\n",
    "l2norm_kernel = cp.ReductionKernel(\n",
    "    'T x',  # input params\n",
    "    'T y',  # output params\n",
    "    'x * x',  # map\n",
    "    'a + b',  # reduce\n",
    "    'y = sqrt(a)',  # post-reduction map\n",
    "    '0',  # identity value\n",
    "    'l2norm'  # kernel name\n",
    ")\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2, 5)\n",
    "l2norm_kernel(x, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntj_EppSZSQC"
   },
   "source": [
    "**Raw kernel**\n",
    "\n",
    "Raw kernels can be defined by the RawKernel class. By using raw kernels, you can define kernels from raw CUDA source.\n",
    "\n",
    "RawKernel object allows you to call the kernel with CUDA cuLaunchKernel interface. In other words, you have control over grid size, block size, shared memory size and stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8w5hJhqCcw4j",
    "outputId": "37c15205-53e9-4df6-f4fe-df7cf4e86d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24.,\n",
       "       26., 28., 30., 32., 34., 36., 38., 40., 42., 44., 46., 48.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import math\n",
    "\n",
    "add_kernel = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void my_add(const float* x1, const float* x2, float* y) {\n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    y[tid] = x1[tid] + x2[tid];\n",
    "}\n",
    "''', 'my_add')\n",
    "\n",
    "\n",
    "x1 = cp.arange(25, dtype=cp.float32)\n",
    "print(x1)\n",
    "x2 = cp.arange(25, dtype=cp.float32)\n",
    "y = cp.zeros(25, dtype=cp.float32)\n",
    "threads_per_block = 512\n",
    "size=25\n",
    "grid_size = (int(math.ceil(size / threads_per_block)), 1, 1)\n",
    "block_size = (threads_per_block, 1, 1)\n",
    "add_kernel(grid_size, block_size, (x1, x2, y, size))\n",
    "#add_kernel((5,), (5,), (x1, x2, y))\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ieEwajBdmRo"
   },
   "source": [
    "## Kernel fusion\n",
    "\n",
    "cupy.fuse() is a decorator that fuses functions. This decorator can be used to define an elementwise or reduction kernel more easily than ElementwiseKernel or ReductionKernel.\n",
    "\n",
    "At the first function call, the fused function analyzes the original function based on the abstracted information of arguments (e.g. their dtypes and ndims) and creates and caches an actual CUDA kernel. From the second function call with the same input types, the fused function calls the previously cached kernel, so it is highly recommended to reuse the same decorated functions instead of decorating local functions that are defined multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kQVvcBudvfq"
   },
   "outputs": [],
   "source": [
    "#By using this decorator, we can define the squared_diff kernel as follows:\n",
    "\n",
    "@cp.fuse(kernel_name='squared_diff')\n",
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "\n",
    "#cupy.fuse() supports simple reduction kernels\n",
    "@cp.fuse()(kernel_name='sum_of_products')\n",
    "def sum_of_products(x, y):\n",
    "    return cp.sum(x * y, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZhjE9P_ettz"
   },
   "source": [
    "**JIT kernel definition**\n",
    "\n",
    "CuPy’s JIT compiler generates CUDA code via Python AST.\n",
    "The cupyx.jit.rawkernel decorator can create raw CUDA kernels from Python functions.\n",
    "\n",
    "In this section, a Python function wrapped with the decorator is called a target function.\n",
    "\n",
    "A target function consists of elementary scalar operations, and users have to manage how to parallelize them. CuPy’s array operations which automatically parallelize operations (e.g., add(), sum()) are not supported. If a custom kernel based on such array functions is desired, please refer to the Kernel fusion section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Hraua0VetBP",
    "outputId": "b98386f3-dd54-4c77-8a9d-931198636097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.7807698   0.09125482 -0.75306845 ... -0.7375061  -0.47575653\n",
      "  0.5274892 ]\n",
      "[ 1.7807698   0.09125482 -0.75306845 ... -0.7375061  -0.47575653\n",
      "  0.5274892 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/cupyx/jit/_interface.py:161: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    }
   ],
   "source": [
    "from cupyx import jit\n",
    "\n",
    "@jit.rawkernel()\n",
    "def elementwise_copy(x, y, size):\n",
    "    tid = jit.blockIdx.x * jit.blockDim.x + jit.threadIdx.x\n",
    "    ntid = jit.gridDim.x * jit.blockDim.x\n",
    "    for i in range(tid, size, ntid):\n",
    "        y[i] = x[i]\n",
    "\n",
    "size = cupy.uint32(2 ** 22)\n",
    "x = cupy.random.normal(size=(size,), dtype=cupy.float32)\n",
    "y = cupy.empty((size,), dtype=cupy.float32)\n",
    "\n",
    "elementwise_copy((128,), (1024,), (x, y, size))  # RawKernel style\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "assert (x == y).all()\n",
    "\n",
    "elementwise_copy[128, 1024](x, y, size)  #  Numba style\n",
    "assert (x == y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGkzIWjyfqYV"
   },
   "source": [
    "# Exercise 1:\n",
    "\n",
    "Implement a cuda kernel that, given two input arrays $(x_1, x_2)$ of size N, produces an output array given by\n",
    "\n",
    "$y[i] = x_1[i]*x_2[i]+ B$\n",
    "\n",
    "where B is a constant\n",
    "\n",
    "Time this kernel function using cuda events for different sizes of the input arrays $(10^4, 10^5, 10^6)$\n",
    "and for different block-grid choices.\n",
    "\n",
    "Produce a plots with the measured times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGes0OUQfuGD"
   },
   "source": [
    "# Exercise 2:\n",
    "\n",
    "Compare the time of cupy matmul with the matmul obtained with a vanilla CUDA matmul implemented with raw kernels.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
