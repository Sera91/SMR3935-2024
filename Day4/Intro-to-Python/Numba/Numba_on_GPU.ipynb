{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Numba’s CUDA support exposes facilities to declare and manage this hierarchy of threads. The facilities are largely similar to those exposed by NVidia’s CUDA C language.\n",
        "\n",
        "Numba also exposes three kinds of GPU memory: global device memory (the large, relatively slow off-chip memory that’s connected to the GPU itself), on-chip shared memory and local memory. For all but the simplest algorithms, it is important that you carefully consider how to use and access memory in order to minimize bandwidth requirements and contention."
      ],
      "metadata": {
        "id": "zewkp4-wEtph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel in NUMBA\n",
        "A kernel function is a GPU function that is meant to be called from CPU code (*). It gives it two fundamental characteristics:\n",
        "\n",
        "kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array);\n",
        "\n",
        "kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n",
        "\n",
        "At first sight, writing a CUDA kernel with Numba looks very much like writing a JIT function for the CPU.\n",
        "\n",
        "To instantiated/invoke the kernel, we need to specify:\n",
        "- a number of blocks (\"blocks per grid\")\n",
        "- a number of threads per block (block size)\n",
        "\n",
        "NOTE: the block size determines how many threads share a given area of shared memory.\n",
        "\n",
        "The product of these 2 numbers will give the total number of threads associated to the running kernel.\n",
        "\n",
        "\n",
        "NOTE: Kernels run asynchronously: launches queue their execution on the device and then return immediately. You can use cuda.synchronize() to wait for all previous kernel launches to finish executing."
      ],
      "metadata": {
        "id": "cKmFHh9SE3kx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX7D2-olpvCR",
        "outputId": "9b36499e-5f0b-473d-de09-3eb6cc8e553a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 42.57 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1.54 ms ± 3.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 20 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "import numba\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    # Compute flattened index inside the array\n",
        "    pos = tx + ty * bw\n",
        "    if pos < an_array.size:  # Check array boundaries\n",
        "        an_array[pos] += 1\n",
        "\n",
        "#threadIdx, blockIdx, blockDim and gridDim are special objects provided by the CUDA backend for the sole purpose of knowing the geometry of the thread hierarchy and the position of the current thread within that geometry.\n",
        "#These objects can be 1D, 2D or 3D, depending on how the kernel was invoked. To access the value at each dimension, use the x, y and z attributes of these objects, respectively.\n",
        "\n",
        "\n",
        "#Kernel invocation\n",
        "\n",
        "an_array= cp.ones(10000, dtype=cp.float32)\n",
        "threadsperblock = 512\n",
        "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock\n",
        "%timeit -n 10 increment_by_one[blockspergrid, threadsperblock](an_array)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
        "\n",
        "- numba.cuda.grid(ndim): Return the absolute position of the current thread in the entire grid of blocks. ndim should correspond to the number of dimensions declared when instantiating the kernel.\n",
        "- numba.cuda.gridsize(ndim):Return the absolute size (or shape) in threads of the entire grid of blocks. ndim has the same meaning as in grid() above."
      ],
      "metadata": {
        "id": "3TKkgF4UGENM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    pos = cuda.grid(1)\n",
        "    if pos < an_array.size:\n",
        "        an_array[pos] += 1\n",
        "\n",
        "@cuda.jit\n",
        "def increment_a_2D_array(an_array):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
        "       an_array[x, y] += 1\n",
        "\n",
        "\n",
        "an_array=cp.random.random((1000,1000), dtype=cp.float32)\n",
        "threadsperblock = (16, 16)\n",
        "blockspergrid_x = math.ceil(an_array.shape[0] / threadsperblock[0])\n",
        "blockspergrid_y = math.ceil(an_array.shape[1] / threadsperblock[1])\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "increment_a_2D_array[blockspergrid, threadsperblock](an_array)\n",
        "%timeit -n 100 increment_a_2D_array[blockspergrid, threadsperblock](an_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Y8WIfxGVA2",
        "outputId": "60988cf0-81ba-4c42-9a66-e04b7cbe9baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236 µs ± 18.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matmul with Numba kernel on GPU"
      ],
      "metadata": {
        "id": "4jQqKK3MGgCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below you find an example of a matmul based on Numba kernels."
      ],
      "metadata": {
        "id": "LL5RaRbJVHCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FASTER MATMUL\n",
        "from __future__ import division\n",
        "from numba import cuda, float32\n",
        "import numpy\n",
        "import math\n",
        "\n",
        "# Controls threads per block and shared memory usage.\n",
        "# The computation will be done on blocks of TPBxTPB elements.\n",
        "TPB = 16\n",
        "\n",
        "@cuda.jit\n",
        "def fast_matmul(A, B, C):\n",
        "    \"\"\"\n",
        "    Perform matrix multiplication of C = A * B\n",
        "    Each thread computes one element of the result matrix C\n",
        "    \"\"\"\n",
        "\n",
        "    # Define an array in the shared memory\n",
        "    # The size and type of the arrays must be known at compile time\n",
        "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
        "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
        "\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "\n",
        "    if x >= C.shape[0] and y >= C.shape[1]:\n",
        "        # Quit if (x, y) is outside of valid C boundary\n",
        "        return\n",
        "\n",
        "    # Each thread computes one element in the result matrix.\n",
        "    # The dot product is chunked into dot products of TPB-long vectors.\n",
        "    tmp = 0.\n",
        "    for i in range(int(A.shape[1] / TPB)):\n",
        "        # Preload data into shared memory\n",
        "        sA[tx, ty] = A[x, ty + i * TPB]\n",
        "        sB[tx, ty] = B[tx + i * TPB, y]\n",
        "\n",
        "        # Wait until all threads finish preloading\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Computes partial product on the shared memory\n",
        "        for j in range(TPB):\n",
        "            tmp += sA[tx, j] * sB[j, ty]\n",
        "\n",
        "        # Wait until all threads finish computing\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    C[x, y] = tmp\n",
        "\n",
        "# The data array\n",
        "A = numpy.full((TPB*2, TPB*3), 3, dtype=numpy.float32) # [32 x 48] matrix containing all 3's\n",
        "B = numpy.full((TPB*3, TPB*1), 4, dtype=numpy.float32) # [48 x 16] matrix containing all 4's\n",
        "\n",
        "A_global_mem = cuda.to_device(A)\n",
        "B_global_mem = cuda.to_device(B)\n",
        "C_global_mem = cuda.device_array((TPB*2, TPB*1)) # [32 x 16] matrix result\n",
        "\n",
        "# Configure the blocks\n",
        "threadsperblock = (TPB, TPB)\n",
        "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[1]))\n",
        "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[0]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "# Start the kernel\n",
        "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
        "res = C_global_mem.copy_to_host()\n",
        "\n",
        "res_test= numpy.matmul(A,B)\n",
        "\n",
        "print('results of Numba matmul:', res)\n",
        "\n",
        "print('results of numpy matmul:', res_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB-aFLSzIw8R",
        "outputId": "0cdd7bd5-12a5-4134-aabb-d55c03a97acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of Numba matmul: [[576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]]\n",
            "results of numpy matmul: [[576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]\n",
            " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
            "  576. 576.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    }
  ]
}